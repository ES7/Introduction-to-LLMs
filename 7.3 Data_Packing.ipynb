{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e93b5be4-1d2f-4f81-b603-dc5317dc9922",
   "metadata": {},
   "source": [
    "# Data Packaging\n",
    "Let’s try to package our training data so that we can upload it on hugging face. First we need to tokenize the data, as LLMs do not work directly on text, their internal calculations require numbers. Then we will pack them, packing tokens into the maximum sequence length to improve training efficiency. While packing we additionally add some special tokens in the beginning and at the end of the sentence.\n",
    "\n",
    "\n",
    "## 1. Tokenizing and creating input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42fd6984-9ef5-49ef-97f0-3bf6d61ab5ec",
   "metadata": {
    "height": 149
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b941e66d624c6faa86a2b1b8e3341c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 40474\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(\n",
    "    \"parquet\", \n",
    "    data_files=\"./data/preprocessed_dataset.parquet\", \n",
    "    split=\"train\"\n",
    ")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c26b86b",
   "metadata": {},
   "source": [
    "Use the `shard` method of the Hugging Face `Dataset` object to split the dataset into 10 smaller pieces, or *shards* (think shards of broken glass). Read more about sharding at [this link](https://huggingface.co/docs/datasets/en/process#shard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be9222e8-427c-472c-9be3-d637e708ec77",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 4048\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.shard(num_shards=10, index=0)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7eaaf4",
   "metadata": {},
   "source": [
    "Load the tokenizer and try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dc1e279-a00b-495f-be17-c349278f60fa",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_path_or_name = \"./models/upstage/SOLAR-10.7B-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path_or_name, \n",
    "    use_fast=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb71f812-1196-46e4-849b-265a98060fbd",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁I', \"'\", 'm', '▁a', '▁short', '▁sentence']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"I'm a short sentence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148df352",
   "metadata": {},
   "source": [
    "Create a helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e62159f4-9764-4685-9353-1d2555a9fe48",
   "metadata": {
    "height": 353
   },
   "outputs": [],
   "source": [
    "def tokenization(example):\n",
    "    # Tokenize\n",
    "    tokens = tokenizer.tokenize(example[\"text\"])\n",
    "\n",
    "    # Convert tokens to ids\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # Add <bos>, <eos> tokens to the front and back of tokens_ids \n",
    "    # bos: begin of sequence, eos: end of sequence\n",
    "    token_ids = [\n",
    "        tokenizer.bos_token_id] \\\n",
    "        + token_ids \\\n",
    "        + [tokenizer.eos_token_id\n",
    "    ]\n",
    "    example[\"input_ids\"] = token_ids\n",
    "\n",
    "    # We will be using this column to count the total number of tokens \n",
    "    # in the final dataset\n",
    "    example[\"num_tokens\"] = len(token_ids)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a956459d",
   "metadata": {},
   "source": [
    "Tokenize all the examples in the pretraining dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ce5581f-003c-44f7-abb7-6b004c53dc5d",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f87f12b59340dfb507e7f9a627cb8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4048 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'input_ids', 'num_tokens'],\n",
      "    num_rows: 4048\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(tokenization, load_from_cache_file=False)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38a85094-0454-4123-b346-e4c65b919508",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text The Colorado Climate Center pr\n",
      "\n",
      "input_ids [1, 415, 15837, 1366, 3314, 6064, 5312, 430, 19102, 304, 1178, 356, 281, 3928, 28725, 9735, 28713, 28725, 264, 1052, 14455, 4623, 28725, 9390, 1452, 274, 28725, 17268, 28713, 28725]\n",
      "\n",
      "num_tokens 549\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[3]\n",
    "\n",
    "print(\"text\", sample[\"text\"][:30]) # \n",
    "print(\"\\ninput_ids\", sample[\"input_ids\"][:30])\n",
    "print(\"\\nnum_tokens\", sample[\"num_tokens\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a989d139",
   "metadata": {},
   "source": [
    "Check the total number of tokens in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17587a90-a707-465e-868d-75ea7b647dfb",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5113663"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.sum(dataset[\"num_tokens\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f8b1a8-f9ec-40d4-8908-370062183b2b",
   "metadata": {},
   "source": [
    "## 2. Packing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0414042b",
   "metadata": {},
   "source": [
    "![Packing data for training](Images/data_packing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386325db",
   "metadata": {},
   "source": [
    "Concatenate input_ids for all examples into a single list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61d75e13-9038-4c4b-9bf4-8230260dcc54",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5113663\n"
     ]
    }
   ],
   "source": [
    "input_ids = np.concatenate(dataset[\"input_ids\"])\n",
    "print(len(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40ece0a4-663c-40fb-a468-ca1d0e9b8b15",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "max_seq_length = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7be52b2b-1ef4-447c-ad16-eb96e0bc84d8",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5113632\n"
     ]
    }
   ],
   "source": [
    "total_length = len(input_ids) - len(input_ids) % max_seq_length\n",
    "print(total_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0791a4fd",
   "metadata": {},
   "source": [
    "Discard extra tokens from end of the list so number of tokens is exactly divisible by `max_seq_length`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e71708c6-8362-4b54-a954-c76614df428a",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5113632,)\n"
     ]
    }
   ],
   "source": [
    "input_ids = input_ids[:total_length]\n",
    "print(input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b08a247-94b0-451f-928b-12c28501d47c",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159801, 32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_reshaped = input_ids.reshape(-1, max_seq_length).astype(np.int32)\n",
    "input_ids_reshaped.shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3189b509-ce16-4a48-a518-e76c230bd637",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(input_ids_reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58c115f",
   "metadata": {},
   "source": [
    "Convert to Hugging Face dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10becde8-3917-41d5-b299-cceacaa95cef",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids'],\n",
      "    num_rows: 159801\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "input_ids_list = input_ids_reshaped.tolist()\n",
    "packaged_pretrain_dataset = datasets.Dataset.from_dict(\n",
    "    {\"input_ids\": input_ids_list}\n",
    ")\n",
    "print(packaged_pretrain_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaec4dd",
   "metadata": {},
   "source": [
    "## 3. Save the packed dataset to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57506c4b-ac2d-4e2c-9d16-c8cea8f995b3",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4144c74661a44dfa1ccaafe91148980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/160 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "21093732"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packaged_pretrain_dataset.to_parquet(\"./data/packaged_pretrain_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9c0266-3679-4a48-a53d-17201d6b3222",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
