# Object Detection
In the provious notebook we saw how to create masks via points and bounding boxes. Here we will see how we can use **natural text** to generate these masks. In order to do this we are going to use a pipeline of models, the output of the first model will be fed into the second model. The first model in this pipeline will be a **zero-shot object detection model**, which will then be followed by a SAM model that will take the generated bounding box from the zero-shot object detection.<br>
The zero-shot object detection model we will be using is called **OWL-ViT**. This model can detect objects within an image based on simple text prompts. The fact that it is a zero-shot model, means that we don't need to train it in any way for it to detect any object within an image. The way we will be using OWL-ViT within this is by using a text prompt that is a string of text to generate a bounding box.<br>
<img src="Images/owlvit.png" width="600" height="800"><br>
The OWL-ViT was trained on two tasks, one pre-training phase and a second fine tuning phase. In the pre-training phase,the model learns to associate an image with a piece of text using a technique that leverages contrastive loss, and this process allows the OWL-ViT model to develop a strong understanding of both an image and its corresponding text. In order to achieve good performance, it also required a fine tuning stage. During this stage the model is trained specifically for object detection. While in the pre-training phase the model was just learning how to associate a piece of text and an image, during the fine tuning stage, the model learns to identify, object and associate them with a particular word or string.<br>

We will create a Comet experiment which will allow us to compare the generated masks that will be produced at the end of this pipeline.
```python
import comet_ml
comet_ml.init(anonymous=True, project_name="3: OWL-ViT + SAM")
exp = comet_ml.Experiment()
```
Here we have created an anonymous Comet experiment means we do not need to create a Comet account to get access to the Comet functionality.
```python
from PIL import Image
logged_artifact = exp.get_artifact("Images", "anmorgan24")
local_artifact = logged_artifact.download("./")

raw_image = Image.open("Images/dog.jpg")
```
We will be using Comet artifacts to load all the images.

```python
from transformers import pipeline
OWL_checkpoint = "google/owlvit-base-patch32"
```
In order to access the OWL-ViT model we will be using the Transformers library and specifically the pipeline method.
```python
# Load the model
detector = pipeline(
    model= OWL_checkpoint,
    task="zero-shot-object-detection"
)
```
First specify what you want to identify in the image via prompt.
```python
prompt = "dog"
```
Then pass this prompt to the detector model that you have just loaded to generate the bounding boxes of the dogs in the image. Pass the prompt in **list** format as it is possible to pass in multiple text prompts.
```python
output = detector(
    raw_image,
    candidate_labels = [prompt]
)
output
```
```python
OUTPUT -->
[{'score': 0.19028441607952118,
  'label': 'dog',
  'box': {'xmin': 888, 'ymin': 376, 'xmax': 2280, 'ymax': 2432}},
 {'score': 0.17161825299263,
  'label': 'dog',
  'box': {'xmin': 2026, 'ymin': 728, 'xmax': 3116, 'ymax': 2436}}]
```
The model identifies both images for the label dog in two different positions.
```python
from utils import preprocess_outputs
input_scores, input_labels, input_boxes = preprocess_outputs(output)
from utils import show_boxes_and_labels_on_image
```
The **`preprocess_outputs`** function simply takes the output above and reformat it to make it easier to plot the bounding boxes on the image.
```python
# Show the image with the bounding boxes
show_boxes_and_labels_on_image(
    raw_image,
    input_boxes[0],
    input_labels,
    input_scores
)
```
<img src="Images/ob1.png" width="400" height="300"><br>
The model has been able to take a text input dog and generate two bounding boxes highlighting each dog on the image. It was even able to overlap the bounding boxes of the two dogs. So based on a simple text prompt we have successfully generated bounding boxes on both the dogs.

### Get Segmentation Masks using Mobile SAM
Now we will use the bounding boxes to get the segmented masks. Instead of using **Fast SAM** as in the previous notebook we will use **Mobile SAM** model. The Mobile SAM model has been optimized to run on devices that might not have access to GPUs.<br>
<img src="Images/mobilesam.png" width="500" height="200"><br>
In order to perform more efficiently, it works on **Model Distillation** which allows us to take a very large model and transfer its knowledge to a smaller model, and this allows us to run the model a lot more efficiently. Model Distillation is a different technique compared to model compression techniques or quatization in the sense that it doesn't actually change the model format, but trains an entirely new and much smaller model.
```python
from ultralytics import SAM
```
```python
SAM_version = "mobile_sam.pt"
model = SAM(SAM_version)
```
Info about [mobile_sam.pt](https://docs.ultralytics.com/models/mobile-sam/)<br>
similar to the previous notebook we will define labels to specify the bounding box belongs to the object we would like to segment or the background.
```python
import numpy as np
labels = np.repeat(1, len(output))
labels #--> array([1, 1])
```
As we are making a pipeline in which the output of first model is fed into the next model, we will use the **`.repeat()`** method of NumPy to create an array of ones with the dimensions same as the output. We can then use the **raw image, bounding boxes** and **labels** to generate the segmented mask.
```python
result = model.predict(
    raw_image,
    bboxes=input_boxes[0],
    labels=labels
)
```
```python
OUTPUT -->
0: 1024x1024 16282.0ms
Speed: 7.9ms preprocess, 16282.0ms inference, 200.6ms postprocess per image at shape (1, 3, 1024, 1024)
```
```python
result
```
Within a second we were able to generate the mask using the highly optimized Mobile SAM model. The **`predict()`** function returns a result object that contains not just the masks but also the original image and additional metadata. We can extract the mask from the result using the below given code.
```python
masks = result[0].masks.data
masks
```
The Mask is simply a series of **False** and **True** booleans, which indicates whether a particular pixel belongs to the background or the generated mask respectively. To visualize the masks we will use the function from the utils library.
```python
from utils import show_masks_on_image

show_masks_on_image(
    raw_image,
    masks
)
```
<img src="Images/ob2.png" width="400" height="300"><br>
We can now combine these two models to get the pipeline. We will implement this technique on another usecase of blurring people faces.

## Image Editing: Blur out faces
```python
image_path = "Images/people.jpeg"
raw_image = Image.open(image_path)
raw_image
```
<img src="Images/people.png" width="400" height="300"><br>
Resize the image to 600 pixels wide. This will allow the entire pipeline to perform in a more time efficient manner.
```python
width = 600
percent = width / float(raw_image.size[0])
hsize = int(float(raw_image.size[1]) * wpercent)
```
### Detect Faces
First step will be how to define the prompt. We will be using text prompt `"human face"`.
```python
candidate_labels = ["human face"]
```
As we are starting a new model pipeline, create a new Comet experiment to keep track of all of the different steps of this pipeline.
```python
exp = comet_ml.Experiment()
```
```python
_ = exp.log_image(
    raw_image,
    name = "Raw image"
)
```
We can now use the OWL-ViT model to create the bounding boxes of the faces in the image.
```python
output = detector(
    raw_image,
    candidate_labels=candidate_labels
)
```
```python
input_scores, input_labels, input_boxes = preprocess_outputs(output)
input_scores
```
Inorder to make each steps easy we will log the image and the bounding boxes to the command platform. 
```python
metadata = {
    "OWL prompt": candidate_labels,
    "SAM version": SAM_version,
    "OWL Version": OWL_checkpoint
}
```
```python
from utils import make_bbox_annots

annotations = make_bbox_annots(
    input_scores,
    input_labels,
    input_boxes,
    metadata
)

_ = exp.log_image(
    raw_image,
    annotations= annotations,
    metadata=metadata,
    name= "OWL output"
)
```
### Segmentation Masks
```python
result = model.predict(
    image_path_resized,
    bboxes=input_boxes[0],
    labels=np.repeat(1, len(input_boxes[0]))
)
```
```python
OUTPUT -->
image 1/1 /home/jovyan/work/people_resized.jpg: 1024x1024 20724.8ms
Speed: 5.4ms preprocess, 20724.8ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)
```
### Blur entire image first
```python
from PIL.ImageFilter import GaussianBlur
blurred_img = raw_image.filter(GaussianBlur(radius=5))
blurred_img
```
<img src="Images/blur.png" width="400" height="300"><br>
We can now use the blur image to repalce just the parts of the image.
```python
masks = result[0].masks.data.cpu().numpy()
total_mask = np.zeros(masks[0].shape)
for mask in masks:
    total_mask = np.add(total_mask,mask)
```

``python
output = np.where(
    np.expand_dims(total_mask != 0, axis=2),
    blurred_img,
    raw_image
)
```
```python
import matplotlib.pyplot as plt
plt.imshow(output)
```
<img src="Images/blurface.png" width="400" height="300"><br>
We can log this image to the Comet platform.
```python
metadata = {
    "OWL prompt": candidate_labels,
    "SAM version": SAM_version,
    "OWL version": OWL_checkpoint
}

_ = exp.log_image(
    output,
    name="Blurred masks",
    metadata = metadata,
    annotations=None
)
```
## Blur just faces of those not wearing sunglasses
```python
candidate_labels = ["a person without sunglasses"]
exp = comet_ml.Experiment()
_ = exp.log_image(raw_image, name="Raw image")
```
```python
output = detector(raw_image, candidate_labels=candidate_labels)
input_scores, input_labels, input_boxes = preprocess_outputs(output)
input_scores
```

```python
from utils import make_bbox_annots
metadata = {
    "OWL prompt": candidate_labels,
    "SAM version": SAM_version,
    "OWL version": OWL_checkpoint,
}

annotations = make_bbox_annots(
    input_scores,
    input_labels,
    input_boxes,
    metadata
)

_ = exp.log_image(
    raw_image,
    annotations=annotations,
    metadata=metadata,
    name="OWL output no sunglasses"
)

result = model.predict(
    image_path_resized,
    bboxes=input_boxes[0],
    labels=np.repeat(1, len(input_boxes[0]))
)
```
```python
blurred_img = raw_image.filter(GaussianBlur(radius=5))
masks = result[0].masks.data.cpu().numpy()

total_mask = np.zeros(masks[0].shape)
for mask in masks:
    total_mask = np.add(total_mask, mask)

output = np.where(
    np.expand_dims(total_mask != 0, axis=2),
    blurred_img,
    raw_image
)
plt.imshow(output)
```
<img src="Images/blurglass.png" width="400" height="300"><br>
By running all the steps of the pipeline we can see that the model have replaced ony the sunglasses in the image, rather what we had originally intended. We can see the model is not performing well so let's switch over to the COmet platform to better understand where the model is failing.
```python
metadata = {
    "OWL prompt": candidate_labels,
    "SAM version": SAM_version,
    "OWL version": OWL_checkpoint,
}

_ = exp.log_image(
    output,
    name="Blurred masks no sunglasses",
    metadata=metadata,
    annotations=None
)
```
After running the above code you will be entered in the Comet platform. Here we will comapre all the images generated side-by-side. To do this add a new panel **(Image Panel)**. In the Image Panel choose the image we want to compare that is: **Raw Image, OWL Image** and **Blurred Image**.<br>
<img src="Images/task1.png" width="400" height="600"><br>
We can see the OWL model has successfully identified all the different faces in the image. As a result the SAM model was able to segment each face and replace it with its blurred version.<br>
<img src="Images/task2.png" width="400" height="600"><br>
For the second task the OWL model has created a bounding box around the sunglasses rather than on faces and hence the SAM model blurred out the sunglasses not the faces.
